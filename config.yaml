# Base language model to fine-tune with GRPO
model_name: "ibm-granite/granite-4.0-h-350m-base"

# IR / retrieval-specific settings
ir:
  sparse_model_name: "naver/splade-cocondenser-ensembledistil"
  nfcorpus_split: "train"        # which qrels split to use: train | dev | test
  max_queries: null              # limit for debugging (set to null to use all)

# GRPOConfig arguments (see GRPOConfig docs)
grpo:
  output_dir: "outputs/nfcorpus-grpo"
  overwrite_output_dir: true

  do_train: true
  do_eval: false

  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1
  num_generations: 16
  generation_batch_size: null

  learning_rate: 3.0e-6
  weight_decay: 0.0
  num_train_epochs: 1.0
  max_steps: -1

  logging_strategy: "steps"
  logging_steps: 1
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 2

  bf16: true
  fp16: false
  gradient_checkpointing: true

  remove_unused_columns: false

  # GRPO-specific bits
  max_prompt_length: 256
  max_completion_length: 128
  #cache_implementation: "static"
  mask_truncated_completions: false

  beta: 0.0                   # no KL by default
  loss_type: "dapo"
  scale_rewards: "group"

  # Generation controls
  temperature: 1.0
  top_p: 1.0
  top_k: null
  #repetition_penalty: 1.0

  # vLLM disabled by default
  use_vllm: false

  # Logging / TensorBoard
  report_to: ["tensorboard"]
  logging_dir: "runs/nfcorpus-grpo"
  run_name: "nfcorpus-grpo-splade"

